{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling correction for large choice sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sam Maurer, Dec. 1, 2016\n",
    "\n",
    "1. Replicate synthetic data from Guevara & Ben-Akiva 2013\n",
    "2. Do MNL with and without sampling correction\n",
    "3. Check whether parameter estimates deviate from true values\n",
    "4. Extend to Mixed Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Generate synthetic data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N = 1000 observations\n",
    "- J = 1000 alternatives for all observations (C_n = C)\n",
    "- X = single attribute distributed Uniform(-2,1) for the first 500 alternatives and Uniform(-1,2) for the second half\n",
    "- beta = generic linear taste coefficient, distributed Normal(mu=1.5, sigma=0.8) across the 1000 observations\n",
    "- systematic utility = beta * X\n",
    "- epsilon = error term distributed ExtremeValue(0,1)\n",
    "- random utility = beta * X + epsilon\n",
    "\n",
    "Utility of alternative i for agent n:\n",
    "$$ U_{in} = V_{in} + \\varepsilon_{in} = \\beta_n x_{i} + \\varepsilon_{in} $$\n",
    "\n",
    "Probability that agent n will choose alternative i:\n",
    "$$ L_n(i \\mid \\beta_n, x_n,C_n) = \\frac {e^{V_{in}}} {\\sum_{j \\epsilon C_n} e^{V_{jn}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[-0.35355949  0.1455681  -0.19170987 -0.36535045 -0.7290356 ]\n"
     ]
    }
   ],
   "source": [
    "# Generate attribute x for each of J alternatives\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Start with J << 1000 to speed up runtimes\n",
    "\n",
    "J = 50  # alternatives\n",
    "\n",
    "Xa = 3 * np.random.rand(J/2) - 2  # uniform distribution over [-2, 1]\n",
    "Xb = 3 * np.random.rand(J/2) - 1  # uniform distribution over [-1, 2]\n",
    "\n",
    "X = np.concatenate((Xa, Xb))\n",
    "\n",
    "print len(X)\n",
    "print X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[ 1.5  1.5  1.5  1.5  1.5]\n"
     ]
    }
   ],
   "source": [
    "# Generate taste coefficient beta for each of N agents \n",
    "\n",
    "# For regular MNL, i think we need to use a single value, instead of a \n",
    "# distribution as Guevara & Ben-Akiva used for the mixture model\n",
    "\n",
    "N = 1000  # agents/observations\n",
    "\n",
    "beta = np.zeros(1000) + 1.5\n",
    "# beta = 0.8 * np.random.randn(N) + 1.5\n",
    "\n",
    "print len(beta)\n",
    "print beta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pd.DataFrame(beta).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Generate probability matrix for N agents choosing among J alternatives\n",
    "\n",
    "def probs(n):\n",
    "    ''' \n",
    "    Return list of J probabilities for agent n\n",
    "    '''\n",
    "    b = beta[n]\n",
    "    exps = [np.exp(b*x) for x in X]\n",
    "    sum_exps = np.sum(exps)\n",
    "    return [exp/sum_exps for exp in exps]\n",
    "\n",
    "P = np.array([probs(n) for n in range(N)])\n",
    "    \n",
    "print P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Check that each row sums to 1\n",
    "\n",
    "print np.sum(P, axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[12, 41, 37, 5, 30, 27, 8, 35, 33, 6]\n"
     ]
    }
   ],
   "source": [
    "# Simulate a choice from J alternatives for each of N agents\n",
    "\n",
    "C = [np.random.choice(range(J), p=p) for p in P]\n",
    "\n",
    "print len(C)\n",
    "print C[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Now we have data:\n",
    "\n",
    "- N agents/observations with true taste coefficients in array \"beta\"\n",
    "- J alternatives with single attributes in array \"X\"\n",
    "- N choice outcomes in array \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Alternate procedure for drawing choices\n",
    "\n",
    "Feras suggests simulating random draws of the error term to generate choices, instead of calculating probabilities and sampling the alternatives. He is surprised the estimated beta isn't closer to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use J, X, N, beta from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Generate a utility matrix for N agents choosing among J alternatives\n",
    "\n",
    "U = [[beta[n]*x + np.random.gumbel() for x in X] for n in range(N)]\n",
    "   \n",
    "print len(U)\n",
    "print len(U[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[33, 20, 39, 31, 27, 31, 18, 31, 17, 7]\n"
     ]
    }
   ],
   "source": [
    "# Each agent chooses the alternative with highest utility\n",
    "\n",
    "C = [np.argmax(a) for a in U]\n",
    "\n",
    "print len(C)\n",
    "print C[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From below, results seem the same: estimated beta is not any more precise, but still correct in conjunction with confidence interval. This algorithm is more concise and readable, so will continue using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate beta without sampling, using PyLogit MNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylogit\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Set up an estimation dataset in long format\n",
    "\n",
    "d = [[n, i, int(C[n]==i), X[i]] for n in range(N) for i in range(J)]\n",
    "\n",
    "print len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obs_id  alt_id  choice         x\n",
      "0       0       0       0 -0.353559\n",
      "1       0       1       0  0.145568\n",
      "2       0       2       0 -0.191710\n",
      "3       0       3       0 -0.365350\n",
      "4       0       4       0 -0.729036 \n",
      "\n",
      "             obs_id        alt_id        choice             x\n",
      "count  50000.000000  50000.000000  50000.000000  50000.000000\n",
      "mean     499.500000     24.500000      0.020000      0.113895\n",
      "std      288.677877     14.431014      0.140001      0.871074\n",
      "min        0.000000      0.000000      0.000000     -1.939345\n",
      "25%      249.750000     12.000000      0.000000     -0.413315\n",
      "50%      499.500000     24.500000      0.000000      0.277541\n",
      "75%      749.250000     37.000000      0.000000      0.836287\n",
      "max      999.000000     49.000000      1.000000      1.834007\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(d, columns=['obs_id', 'alt_id', 'choice', 'x'])\n",
    "\n",
    "print df.head(), '\\n'\n",
    "print df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up model spec\n",
    "\n",
    "spec = OrderedDict([\n",
    "        ('x', [range(J)])\n",
    "    ])\n",
    "\n",
    "labels = OrderedDict([\n",
    "        ('x', ['beta_x'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -3,912.0230\n",
      "Initial Log-likelihood: -3,912.0230\n",
      "Estimation Time: 0.05 seconds.\n",
      "Final log-likelihood: -3,303.4579\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                1,000\n",
      "Model:             Multinomial Logit Model   Df Residuals:                      999\n",
      "Method:                                MLE   Df Model:                            1\n",
      "Date:                     Thu, 01 Dec 2016   Pseudo R-squ.:                   0.156\n",
      "Time:                             11:03:25   Pseudo R-bar-squ.:               0.155\n",
      "converged:                            True   Log-Likelihood:             -3,303.458\n",
      "                                             LL-Null:                    -3,912.023\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "beta_x         1.5351      0.048     31.862      0.000         1.441     1.629\n",
      "==============================================================================\n",
      "CPU times: user 7.86 s, sys: 14.2 s, total: 22.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = pylogit.create_choice_model(data = df, \n",
    "                                alt_id_col = 'alt_id', \n",
    "                                obs_id_col = 'obs_id', \n",
    "                                choice_col = 'choice', \n",
    "                                specification = spec, \n",
    "                                model_type = \"MNL\", \n",
    "                                names = labels)\n",
    "\n",
    "m.fit_mle(init_vals = np.array([0]))\n",
    "print m.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try with UrbanSim MNL instead of PyLogit\n",
    "\n",
    "Model class: https://github.com/UDST/urbansim/blob/master/urbansim/models/dcm.py\n",
    "\n",
    "Estimation algorithms: https://github.com/UDST/urbansim/blob/master/urbansim/urbanchoice/mnl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urbansim.models import MNLDiscreteChoiceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Choosers should be a DataFrame of characteristics, with index as identifier\n",
    "\n",
    "d = [[n, C[n]] for n in range(N)]\n",
    "\n",
    "choosers = pd.DataFrame(d, columns=['id', 'choice']).set_index('id')\n",
    "\n",
    "print len(choosers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# Alternatives should be a DataFrame of characteristics, with index as identifier\n",
    "\n",
    "d = [[i, X[i]] for i in range(J)]\n",
    "\n",
    "alts = pd.DataFrame(d, columns=['id', 'x']).set_index('id')\n",
    "\n",
    "print len(alts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Log-liklihood: -3891.820\n",
      "Log-liklihood at convergence: -3299.400\n",
      "Log-liklihood Ratio: 0.152\n",
      "\n",
      "+-----------+-------------+------------+---------+\n",
      "| Component | Coefficient | Std. Error | T-Score |\n",
      "+-----------+-------------+------------+---------+\n",
      "| x         |    1.539    |   0.028    |  55.457 |\n",
      "+-----------+-------------+------------+---------+\n",
      "CPU times: user 116 ms, sys: 9.56 ms, total: 125 ms\n",
      "Wall time: 117 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurer/Dropbox/Git-mbp13/udst/urbansim/urbansim/urbanchoice/interaction.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  alts_sample['join_index'] = np.repeat(choosers.index.values, SAMPLE_SIZE)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# It seems like this implementation *requires* us to sample the alternatives, \n",
    "# so here i'm estimating the model with J-1 alts\n",
    "\n",
    "m = MNLDiscreteChoiceModel(model_expression = 'x',\n",
    "                           sample_size = J-1)\n",
    "\n",
    "m.fit(choosers = choosers,\n",
    "      alternatives = alts,\n",
    "      current_choice = 'choice')\n",
    "\n",
    "m.report_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. MNL, sampling alternatives without correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the estimation dataset, for each observation include a row for the\n",
    "# chosen alternative, plus K-1 other alternatives sampled randomly\n",
    "# without replacement, where K < J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 48, 23]\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "def alts(obs_id):\n",
    "    \"\"\"\n",
    "    Sample alternatives for observation `obs_id`. Expects `J` total\n",
    "    alts, `K` sampled alts, list `C` of choice outcomes. Returns list \n",
    "    of K alt id's including the chosen one.\n",
    "    \"\"\"\n",
    "    chosen = C[obs_id]  # id of chosen alternative\n",
    "    unchosen = [i for i in range(J) if chosen != i]  # id's of J-1 unchosen alts\n",
    "    sample_unchosen = np.random.choice(unchosen, size=K-1, replace=False)\n",
    "    return [chosen] + sample_unchosen.tolist()\n",
    "    \n",
    "print alts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Set up the estimation dataset\n",
    "\n",
    "d = [[n, i, int(C[n]==i), X[i]] for n in range(N) for i in alts(n)]\n",
    "\n",
    "print len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obs_id  alt_id  choice         x\n",
      "0       0      33       1  0.705302\n",
      "1       0      17       0  0.497860\n",
      "2       0      47       0 -0.613221\n",
      "3       1      20       1  0.935855\n",
      "4       1       9       0 -0.849675 \n",
      "\n",
      "            obs_id       alt_id       choice            x\n",
      "count  3000.000000  3000.000000  3000.000000  3000.000000\n",
      "mean    499.500000    26.133000     0.333333     0.408349\n",
      "std     288.723115    13.796597     0.471483     0.900302\n",
      "min       0.000000     0.000000     0.000000    -1.939345\n",
      "25%     249.750000    15.000000     0.000000    -0.206333\n",
      "50%     499.500000    27.000000     0.000000     0.531702\n",
      "75%     749.250000    38.000000     1.000000     1.000300\n",
      "max     999.000000    49.000000     1.000000     1.834007\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(d, columns=['obs_id', 'alt_id', 'choice', 'x'])\n",
    "\n",
    "print df.head(), '\\n'\n",
    "print df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same model spec as before\n",
    "\n",
    "spec = OrderedDict([\n",
    "        ('x', [range(J)])\n",
    "    ])\n",
    "\n",
    "labels = OrderedDict([\n",
    "        ('x', ['beta_x'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -1,098.6123\n",
      "Initial Log-likelihood: -1,098.6123\n",
      "Estimation Time: 0.01 seconds.\n",
      "Final log-likelihood: -758.2954\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                1,000\n",
      "Model:             Multinomial Logit Model   Df Residuals:                      999\n",
      "Method:                                MLE   Df Model:                            1\n",
      "Date:                     Thu, 01 Dec 2016   Pseudo R-squ.:                   0.310\n",
      "Time:                             11:18:42   Pseudo R-bar-squ.:               0.309\n",
      "converged:                            True   Log-Likelihood:               -758.295\n",
      "                                             LL-Null:                    -1,098.612\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "beta_x         1.4542      0.073     19.822      0.000         1.310     1.598\n",
      "==============================================================================\n",
      "CPU times: user 297 ms, sys: 77.5 ms, total: 374 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = pylogit.create_choice_model(data = df, \n",
    "                                alt_id_col = 'alt_id', \n",
    "                                obs_id_col = 'obs_id', \n",
    "                                choice_col = 'choice', \n",
    "                                specification = spec, \n",
    "                                model_type = \"MNL\", \n",
    "                                names = labels)\n",
    "\n",
    "m.fit_mle(init_vals = np.array([0]))\n",
    "print m.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 100x with different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_beta():\n",
    "    d = [[n, i, int(C[n]==i), X[i]] for n in range(N) for i in alts(n)]\n",
    "    df = pd.DataFrame(d, columns=['obs_id', 'alt_id', 'choice', 'x'])\n",
    "    m = pylogit.create_choice_model(df, 'alt_id', 'obs_id', 'choice', spec, 'MNL', names=labels)\n",
    "    m.fit_mle(init_vals = np.array([0]))\n",
    "    return m.params.beta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "beta = []\n",
    "for i in range(1000):\n",
    "    beta.append(estimate_beta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pd.Series(beta).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look very biased, actually: mean 1.509 vs. 1.532 without sampling alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNL with sampling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility of alternative j:\n",
    "$$ V_{j} = \\beta x_{j} $$\n",
    "\n",
    "With sampling, we have to account for the restricted choice set (from Eq 6 in Guevara & Ben-Akiva 2013):\n",
    "\n",
    "$$ V_j = \\beta x_j + \\ln \\pi(D \\mid j) $$\n",
    "\n",
    "Where pi is the conditional probability that we would construct the choice set D given that alternative j was chosen. This goes into the likelihood function in both the numerator and denominator.\n",
    "\n",
    "$$ L_n = \\frac {exp(\\beta x_i + \\ln \\pi(D_n \\mid i))} {\\sum_{j \\epsilon D_n} exp(\\beta x_j + \\ln \\pi(D_n \\mid j))} $$\n",
    "\n",
    "How to calculate pi? From the original formulation of this in McFadden 1978: \"Suppose D is comprized of i plus a sample of alternatives from the set C\\\\{i}, obtained by considering each element of this set independently, and including it with probability p. Then, the probability of D will depend solely on the number of elements K it contains.\"\n",
    "\n",
    "$$ \\pi(D) = p^{K-1} (1 - p)^{J-K} $$\n",
    "\n",
    "(?? Without replacement, i think it should be the n-choose-k binomial coefficient, where n=J-1 and k=K-1)\n",
    "\n",
    "$$ \\pi(D) = {n \\choose k} = \\frac {(K-1)!(J-K)!} {(J-1)!} $$\n",
    "\n",
    "With random sampling, the probability is the same for all j, so we can correct for it without modifying the estimation routine, just by including a constant with fixed value in the model specification:\n",
    "\n",
    "$$ const = \\ln \\pi(D) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obs_id  alt_id  choice         x  const\n",
      "0       0      12       1  0.832675      1\n",
      "1       0      24       0 -1.070307      1\n",
      "2       0       4       0 -1.956275      1\n",
      "3       1      41       1  1.985414      1\n",
      "4       1      26       0  0.413689      1 \n",
      "\n",
      "            obs_id       alt_id       choice            x   const\n",
      "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.0\n",
      "mean    499.500000    26.777667     0.333333     0.438763     1.0\n",
      "std     288.723115    13.883149     0.471483     1.180510     0.0\n",
      "min       0.000000     0.000000     0.000000    -1.993222     1.0\n",
      "25%     249.750000    15.000000     0.000000    -0.343887     1.0\n",
      "50%     499.500000    29.000000     0.000000     0.413689     1.0\n",
      "75%     749.250000    38.000000     1.000000     1.448505     1.0\n",
      "max     999.000000    49.000000     1.000000     1.985414     1.0\n"
     ]
    }
   ],
   "source": [
    "# Add a column in the estimation data for the constant\n",
    "\n",
    "d = [[n, i, int(C[n]==i), X[i], 1] for n in range(N) for i in alts(n)]\n",
    "\n",
    "df = pd.DataFrame(d, columns=['obs_id', 'alt_id', 'choice', 'x', 'const'])\n",
    "\n",
    "print df.head(), '\\n'\n",
    "print df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec2 = OrderedDict([\n",
    "        ('x', [range(J)]),\n",
    "        ('const', [range(J)])\n",
    "    ])\n",
    "\n",
    "labels2 = OrderedDict([\n",
    "        ('x', ['beta_x']),\n",
    "        ('const', ['constant'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try binomial formula\n",
    "\n",
    "j=3\n",
    "k=2\n",
    "\n",
    "fact = np.math.factorial\n",
    "\n",
    "float(fact(k-1)*fact(j-k))/fact(j-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -1,098.6123\n",
      "Initial Log-likelihood: -1,098.6123\n",
      "Estimation Time: 0.02 seconds.\n",
      "Final log-likelihood: -613.3560\n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                1,000\n",
      "Model:             Multinomial Logit Model   Df Residuals:                      998\n",
      "Method:                                MLE   Df Model:                            2\n",
      "Date:                     Mon, 21 Nov 2016   Pseudo R-squ.:                   0.442\n",
      "Time:                             13:47:43   Pseudo R-bar-squ.:               0.440\n",
      "converged:                            True   Log-Likelihood:               -613.356\n",
      "                                             LL-Null:                    -1,098.612\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "beta_x         1.5376      0.075     20.586      0.000         1.391     1.684\n",
      "constant      -7.0699   1.31e+07  -5.39e-07      1.000     -2.57e+07  2.57e+07\n",
      "==============================================================================\n",
      "CPU times: user 325 ms, sys: 29.7 ms, total: 355 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = pylogit.create_choice_model(data = df, \n",
    "                                alt_id_col = 'alt_id', \n",
    "                                obs_id_col = 'obs_id', \n",
    "                                choice_col = 'choice', \n",
    "                                specification = spec2, \n",
    "                                model_type = \"MNL\", \n",
    "                                names = labels2)\n",
    "\n",
    "# p = float(K-1)/(J-1)\n",
    "# const = np.log(p**(K-1) * (1-p)**(J-K))\n",
    "\n",
    "const = np.log(float(fact(K-1)*fact(J-K))/fact(J-1))\n",
    "\n",
    "# Add an initial value for the constant and constrain it to that\n",
    "m.fit_mle(init_vals = np.array([0, const]), \n",
    "          constrained_pos=[1])\n",
    "\n",
    "print m.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 100x with different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try binomial formula\n",
    "const = np.log(float(fact(K-1)*fact(J-K))/fact(J-1))\n",
    "\n",
    "def estimate_beta_with_correction():\n",
    "    d = [[n, i, int(C[n]==i), X[i], 1] for n in range(N) for i in alts(n)]\n",
    "    df = pd.DataFrame(d, columns=['obs_id', 'alt_id', 'choice', 'x', 'const'])\n",
    "    m = pylogit.create_choice_model(df, 'alt_id', 'obs_id', 'choice', spec2, 'MNL', names=labels2)\n",
    "    m.fit_mle(init_vals = np.array([0, const]), constrained_pos=[1])\n",
    "    return m.params.beta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "beta = []\n",
    "for i in range(1000):\n",
    "    beta.append(estimate_beta_with_correction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1000.000000\n",
      "mean        1.513490\n",
      "std         0.051725\n",
      "min         1.354507\n",
      "25%         1.477341\n",
      "50%         1.512756\n",
      "75%         1.548081\n",
      "max         1.736557\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print pd.Series(beta).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why isn't this correcting the bias? 1.513 here vs. 1.509 uncorrected vs. 1.532 without sampling.\n",
    "\n",
    "McFadden (1978): \"Comparison of these functions shows that Friedman's procedure [of maximizing standard MNL likelihood functions despite having sampled alternatives] provides consistent estimates of the parameter vector beta. ... In conclusion, analysis of housing location can be carried out with a limited number of alternatives. ... If a mechanism such as [random sampling] is used to select alternatives, the likelihood function should be modified [to include the sampling correction term] to obtain consistent estimates of all parameters. If a non-modified likelihood function is used, estimation can still be carried out satisfactorily provided the effect of the selection mechanism for alternatives is absorbed by class-specific parameters.\"\n",
    "\n",
    "#### Sam's interpretation at this point:\n",
    "\n",
    "- For pure random sampling, no correction is needed\n",
    "\n",
    "- For random sampling within classes of alternatives, no correction is needed, but we have to include class-specific constants in the specification (per McFadden)\n",
    "\n",
    "- For sampling that's conditional on the chosen alternative, we would need to include a correction term in the likelihood function (and it would have to be done in the estimation routine rather than through a constant, because the probabilities of constructing choice sets would vary) -- but intuitively, targeted sampling seems like it would *reduce* the estimation bias, so maybe no correction is needed here either\n",
    "\n",
    "#### Questions\n",
    "\n",
    "- The bias from sampling alternatives seems small, but why doesn't the correction term remove it? Is the beta estimated without sampling the correct benchmark here?\n",
    "\n",
    "- How can i construct an example that maximizes the potential bias from sampling, to better test the correction terms?\n",
    "\n",
    "- What's our intuition of how sampling would behave differently in ML or LCCM? Is it worth extending this example to a mixture model as in Guevara & Ben-Akiva, to see what the bias is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Does bias disappear with even larger samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do \n",
    "# - look through PyLogit and LCCM code\n",
    "# - in many-alternative scenarios, attirbutes of the alternatives will \n",
    "#   usually be in a separate data table - what helper functions do we need?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
